{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Evaluation of LLM Based QA Systems\n",
    "\n",
    "The following notebook was part of workshop **Automated Evaluation of LLM Based QA systems** presented at Machine Learning Conference 2024. It aims to show you basics of designing RAG application and how to optimize and monitor its performance with evaluator model and curated dataset of questions. Brief description of your goals for the practical session is presented at the start of every session. Notebook is supplemented by a power point presentation also presented in this repository.\n",
    "\n",
    "### Notebook requirements\n",
    "\n",
    "To run the notebook, you need an access to OpenAI API through Azure for a LLM (notebook was designed weith GPT3.5 in mind) and embedding model _text-ada-embedding-002_ which is unfortunately required due to the prepared RAG database already having these embeddings calculated. We also provide notebook `prepare_data.ipynb` for creating the rag database from scratch, so you can implement different embedding model.\n",
    "\n",
    "If you want to implement different models, you have to change langchain functions `AzureChatOpenAI` and `AzureOpenAIEmbeddings` with equivalent counter parts. But don't forget that changing embedding function requires changing of the RAG database!\n",
    "\n",
    "### Authors\n",
    "MlPrague 2024, 22.04.2024 9:00 - 12:30, Workshop lecturers:\n",
    "  - Ondřej Finke, O2 [Dataclair.ai](https://dataclair.ai/), ondrej.finke@o2.cz\n",
    "  - Alexandr Vendl, O2 [Dataclair.ai](https://dataclair.ai/), alexandr.vendl@o2.cz\n",
    "  - Marek Matiáš, O2 [Dataclair.ai](https://dataclair.ai/), marek.matias@o2.cz\n",
    "\n",
    "\n",
    "Practical sessions:\n",
    "  - Session 1: Getting to know the RAG application\n",
    "  - Session 2: Manually evaluating the RAG application\n",
    "  - Session 2: Training the evaluator model\n",
    "  - Session 3: Optimizing the RAG application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 1 - IMPORTS AND SETTINGS\n",
    "# imports\n",
    "import random\n",
    "import time\n",
    "from functools import wraps\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.output_parsers import JsonOutputParser, StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.runnables import (\n",
    "    ConfigurableField,\n",
    "    Runnable,\n",
    "    RunnableLambda,\n",
    "    RunnablePassthrough,\n",
    ")\n",
    "from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "# supporting functions\n",
    "def make_autopct(values):\n",
    "    \"\"\"format of numbers in the pie chart\"\"\"\n",
    "\n",
    "    def my_autopct(pct):\n",
    "        total = sum(values)\n",
    "        val = int(round(pct * total / 100.0))\n",
    "        return f\"{pct:.1f}% ({val:d})\"\n",
    "\n",
    "    return my_autopct\n",
    "\n",
    "\n",
    "def retry_on_exception(\n",
    "    func,\n",
    "    max_retries: int = 3,\n",
    "    initial_delay: float = 1.0,\n",
    "    exponential_base: float = 2.0,\n",
    "    jitter: bool = True,\n",
    "    exceptions: tuple = (Exception,),\n",
    "    on_timeout: Exception = Exception(\"Connection failed, please try again.\"),\n",
    "):\n",
    "    \"\"\"\n",
    "    General decorator function to retry the decorated function on exception.\n",
    "    It retries the function up to `max_retries` times.\n",
    "    \"\"\"\n",
    "\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        retries = 0\n",
    "        delay = initial_delay\n",
    "        while retries < max_retries:\n",
    "            try:\n",
    "                result = func(*args, **kwargs)\n",
    "                return result\n",
    "            except Exception as e:  # modify this line\n",
    "                print(f\"Exception occurred: {e}\")  # add this line\n",
    "                # Increment the delay\n",
    "                delay *= exponential_base * (1 + jitter * random.random())\n",
    "                # Sleep for the delay\n",
    "                time.sleep(delay)\n",
    "                retries += 1\n",
    "        return []\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "# SETTINGS FOR THE LLM MODELS\n",
    "# Add your api keys and endpoints\n",
    "api_key = \"\"\n",
    "api_endpoint = \"\"\n",
    "api_version = \"\"\n",
    "ans_model_type = \"gpt-35-turbo\"             # LLM for answering\n",
    "emb_model_type = \"text-embedding-ada-002\"   # embedding model. ADA-2 was used in db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Session 1: Getting to know the RAG application\n",
    "\n",
    "During the first practical session, your goal is to get familiar with the presented implementation of RAG application on top GPT3.5 model. RAG chain is build as a langchain Runnable which, when called, does the following:\n",
    "1) Retrieve documents from vector database based on user question\n",
    "2) Select documents according to the search type and hyperparameters\n",
    "3) Generate response based on the found documents\n",
    "\n",
    "**Information about the data**\n",
    "\n",
    "- 567 documents present in the database from 22 Wikipedia articles\n",
    "- Articles are scraped in January 2024, please, follow these links for the january revision of the article\n",
    "  - [2024 United States presidential election](https://en.wikipedia.org/w/index.php?title=2024_United_States_presidential_election&oldid=1200006968)\n",
    "  - [2024 Taiwanese presidential election](https://en.wikipedia.org/w/index.php?title=2024_Taiwanese_presidential_election&oldid=1199911440)\n",
    "  - [2023 Turkish presidential election](https://en.wikipedia.org/w/index.php?title=2023_Turkish_presidential_election&oldid=1198592810)\n",
    "  - [2023 Turkish parliamentary election](https://en.wikipedia.org/w/index.php?title=2023_Turkish_parliamentary_election&oldid=1199995550)\n",
    "  - [2023 Slovak parliamentary election](https://en.wikipedia.org/w/index.php?title=2023_Slovak_parliamentary_election&oldid=1199500182)\n",
    "  - [2023 Singaporean presidential election](https://en.wikipedia.org/w/index.php?title=2023_Singaporean_presidential_election&oldid=1196788263)\n",
    "  - [2023 Serbian parliamentary election](https://en.wikipedia.org/w/index.php?title=2023_Serbian_parliamentary_election&oldid=1195591108)\n",
    "  - [2023 Polish parliamentary election](https://en.wikipedia.org/w/index.php?title=2023_Polish_parliamentary_election&oldid=1196759103)\n",
    "  - [2023 Finnish parliamentary election](https://en.wikipedia.org/w/index.php?title=2023_Finnish_parliamentary_election&oldid=1200416522)\n",
    "  - [2023 Estonian parliamentary election](https://en.wikipedia.org/w/index.php?title=2023_Estonian_parliamentary_election&oldid=1199658758)\n",
    "  - [2023 Egyptian presidential election](https://en.wikipedia.org/w/index.php?title=2023_Egyptian_presidential_election&oldid=1199675829)\n",
    "  - [2023 Czech presidential election](https://en.wikipedia.org/w/index.php?title=2023_Czech_presidential_election&oldid=1200526766)\n",
    "  - [2023 Bulgarian parliamentary election](https://en.wikipedia.org/w/index.php?title=2023_Bulgarian_parliamentary_election&oldid=1194904336)\n",
    "  - [2022 Turkmenistan presidential election](https://en.wikipedia.org/wiki/2022_Turkmenistan_presidential_election)\n",
    "  - [2022 South Korean presidential election](https://en.wikipedia.org/w/index.php?title=2022_South_Korean_presidential_election&oldid=1189688011)\n",
    "  - [2022 Slovenian parliamentary election](https://en.wikipedia.org/wiki/2022_Slovenian_parliamentary_election)\n",
    "  - [2022 Maltese general election](https://en.wikipedia.org/wiki/2022_Maltese_general_election)\n",
    "  - [2022 Latvian parliamentary election](https://en.wikipedia.org/w/index.php?title=2022_Latvian_parliamentary_election&oldid=1197667621)\n",
    "  - [2022 Hungarian parliamentary election](https://en.wikipedia.org/w/index.php?title=2022_Hungarian_parliamentary_election&oldid=1196216171)\n",
    "  - [2022 French presidential election](https://en.wikipedia.org/w/index.php?title=2022_French_presidential_election&oldid=1200325554)\n",
    "  - [2022 Bulgarian parliamentary election](https://en.wikipedia.org/w/index.php?title=2022_Bulgarian_parliamentary_election&oldid=1193509245)\n",
    "  - [2022 Austrian presidential election](https://en.wikipedia.org/w/index.php?title=2022_Austrian_presidential_election&oldid=1184476052)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 2\n",
    "# PROMPT TEMPLATE FOR THE RAG APPLICATION\n",
    "# template needs to include {context} and {question} where the context from loaded documents\n",
    "# and question is loaded when\n",
    "\n",
    "rag_prompt_template = \"\"\"You are an AI assistant trained for political sciences. You provide an answer to a question solely based on the provided context.\n",
    "Answer in full sentences. At the end of your answer, write name of the article you used as a source in square brackets, like so: [name of the article].\n",
    "If you don't have a context for asked question, reply with the following message: \"I'm sorry, but I cannot find answer to this question in my data.\".\n",
    "Provided articles:\n",
    "{context}\n",
    "\n",
    "Users question: {question}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_rag_chain() -> Runnable:\n",
    "    \"\"\"RAG langchain chain with following steps\n",
    "\n",
    "    1) Retrieve documents based on user question\n",
    "    2) Select top n documents\n",
    "    3) Generate response based on the found documents\n",
    "\n",
    "    When invoking the chain, following rules has to be followed:\n",
    "    Keys for input dictionary:\n",
    "        - question: str - question you want to ask about the data\n",
    "        - search: dict - dictionary defining the search type\n",
    "\n",
    "    Output dictionary:\n",
    "        - answer: str - answer from the LLM\n",
    "        - documents: list[Document] - list of documents retrieved from the database\n",
    "        - context: str - Documents passed into the prompt in the form of merged str\n",
    "    \"\"\"\n",
    "\n",
    "    # define the retriever\n",
    "    # embedding model\n",
    "    embeddings = AzureOpenAIEmbeddings(\n",
    "        api_key=api_key,\n",
    "        azure_endpoint=api_endpoint,\n",
    "        api_version=api_version,\n",
    "        model=emb_model_type,\n",
    "        tiktoken_model_name=\"cl100k_base\",\n",
    "    )\n",
    "    # load chroma db\n",
    "    db = Chroma(persist_directory=\"./data\", embedding_function=embeddings)\n",
    "\n",
    "    # supporting functions\n",
    "    def retrieve_docs(dictionary, db=db) -> list[Document]:\n",
    "        # retrieves documents based on search type\n",
    "        # maximum marginal difference\n",
    "        if dictionary[\"search\"][\"type\"] == \"mmr\":\n",
    "            docs = db.max_marginal_relevance_search(\n",
    "                dictionary[\"question\"],\n",
    "                k=dictionary[\"search\"][\"num_docs\"],\n",
    "                fetch_k=dictionary[\"search\"][\"num_docs\"] * 4,\n",
    "                lambda_mult=dictionary[\"search\"][\"lambda_mult\"],\n",
    "            )\n",
    "        # vector similarity\n",
    "        elif dictionary[\"search\"][\"type\"] == \"vector\":\n",
    "            docs = db.similarity_search_with_relevance_scores(\n",
    "                dictionary[\"question\"], k=dictionary[\"search\"][\"num_docs\"]\n",
    "            )\n",
    "            docs = [\n",
    "                document[0]\n",
    "                for document in docs\n",
    "                if document[1] > dictionary[\"search\"][\"similarity_treshold\"]\n",
    "            ]\n",
    "        return docs\n",
    "\n",
    "    def select_docs(dictionary):\n",
    "        # merge retrieved documents into string used for the prompt based in filters\n",
    "        # context length is = num_docs or less depending on treshold in vector search\n",
    "        context = \"\"\n",
    "        for i in range(len(dictionary[\"documents\"])):\n",
    "            context += f\"Context {i+1}:\\n{dictionary['documents'][i].page_content}\\n\\n\"\n",
    "\n",
    "        if context == \"\":\n",
    "            print(\"WARNING: LLM didn't receive any context\")\n",
    "        return context\n",
    "\n",
    "    # prompt used for the RAG application\n",
    "\n",
    "    # define the model\n",
    "    prompt = ChatPromptTemplate.from_template(rag_prompt_template)\n",
    "    model = AzureChatOpenAI(\n",
    "        openai_api_key=api_key,\n",
    "        azure_endpoint=api_endpoint,\n",
    "        azure_deployment=ans_model_type,\n",
    "        openai_api_version=api_version,\n",
    "        temperature=0,\n",
    "        tiktoken_model_name=\"cl100k_base\",\n",
    "    ).configurable_fields(\n",
    "        temperature=ConfigurableField(\n",
    "            id=\"temp\",\n",
    "            name=\"LLM Temperature\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # define the full chain\n",
    "    chain = (\n",
    "        RunnablePassthrough.assign(documents=RunnableLambda(retrieve_docs))\n",
    "        | RunnablePassthrough.assign(context=RunnableLambda(select_docs))\n",
    "        | RunnablePassthrough.assign(answer=prompt | model | StrOutputParser())\n",
    "    )\n",
    "\n",
    "    return chain\n",
    "\n",
    "\n",
    "# prepare the chain\n",
    "rag_chain = get_rag_chain()\n",
    "\n",
    "\n",
    "# function for calling the chain with retries on exception\n",
    "@retry_on_exception\n",
    "def rag_chain_with_retry(input_dict, temperature=0):\n",
    "    return rag_chain.with_config(configurable={\"temp\": temperature}).invoke(input_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the RAG\n",
    "\n",
    "RAG chain is called using `rag_chain_with_retry` function. Input to this function is `temperature` (default = 0) of the model and `input_dict` which is a dictionary with keys `question` and `search`, where key `search` holds another dictionary defining which type of search is used. Dictionary for the two search types are defined as variables _search_mmr_ and _search_vec_.\n",
    "\n",
    "Output of the chain is a dictionary with keys:\n",
    "- `documents` - list of documents found by the search (langchain Document class)\n",
    "- `context` - documents converted to strings for the llm prompt.\n",
    "- `answer` - string with answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG RUNTIME\n",
    "# question you want to ask the RAG app\n",
    "question = \"Who is the president of Czech Republic?\"\n",
    "\n",
    "# search settings\n",
    "# maximum marginal relevance\n",
    "# optimizes for similarity to query and diversity among selected documents.\n",
    "# lambda_mult <0,1> - degree of diversity between the documents, 1 = minimum diversity\n",
    "search_mmr = {\"type\": \"mmr\", \"num_docs\": 4, \"lambda_mult\": 1}\n",
    "\n",
    "# vector similarity\n",
    "# returns num_docs closest documents based on vector similarity\n",
    "# similarity treshold <0,1> - documents with lower than this similarity will be ignored\n",
    "search_vec = {\"type\": \"vector\", \"num_docs\": 5, \"similarity_treshold\": 0}\n",
    "\n",
    "# input dictionary for the RAG chain\n",
    "rag_input_dict = {\"question\": question, \"search\": search_mmr}\n",
    "\n",
    "# Call the chain\n",
    "rag_answer = rag_chain_with_retry(temperature=0, input_dict=rag_input_dict)\n",
    "\n",
    "# print the answer\n",
    "print(\"\")\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {rag_answer['answer']}\")\n",
    "print(\"-------\")\n",
    "print(f\"Number of documents used: {len(rag_answer['documents'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Session 2: Manual RAG optimization\n",
    "\n",
    "In the second session, try to optimize the RAG performance manually. Look at the subset of provided questions and ground truth answers, generate new answers using your RAG app and try to change the RAG settings to achieve better results. You can change the prompt in variable `rag_prompt_template` above, change search type and it's hyperparameters, number of documents provided in the prompt, and the model temperature. All these will have some effect on the answer quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load 8 questions and nice print the answers\n",
    "mdf = pd.read_csv(\"./data/validation_rag_questions.csv\", sep=\",\")\n",
    "mdf = mdf.sample(8, random_state=42)\n",
    "\n",
    "# answer these questions with rag\n",
    "search_mmr = {\"type\": \"mmr\", \"num_docs\": 4, \"lambda_mult\": 0}\n",
    "search_vec = {\"type\": \"vector\", \"num_docs\": 5, \"similarity_treshold\": 0}\n",
    "\n",
    "mdf[\"rag_answer\"] = \"empty\"\n",
    "for index, row in mdf.iterrows():\n",
    "    try:\n",
    "        # run the chain\n",
    "        rag_input_dict = {\"question\": row[\"question\"], \"search\": search_mmr}\n",
    "        rag_answer = rag_chain_with_retry(temperature=0, input_dict=rag_input_dict)\n",
    "        # save the results\n",
    "        mdf.loc[index, \"rag_answer\"] = rag_answer[\"answer\"]\n",
    "        print(f\"Row {index} answered\")\n",
    "    except Exception as err:\n",
    "        print(f\"WARNING: Row {index}, Exception raised as {err}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update pandas settings to show full answers\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "# show the answers\n",
    "mdf[[\"question\", \"correct_answer\", \"rag_answer\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the pandas settings\n",
    "pd.reset_option(\"display.max_colwidth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Session 3: Evaluator model\n",
    "\n",
    "During the third session, your goal is to train evaluator model which is predicting if the provided `answer` is same as `ground_truth`. This model can be LLM based (as in next cells) or done using classic machine learning methods (for example logistic regression on cosine distance between the ground truth, generated answer and question). For training the evaluator, you will be provided with dataset which includes:\n",
    "  - question\n",
    "  - human_answer - this is the answer created by the human and is considered correct\n",
    "  - rag_answer - answer generated to the same question by LLM (already generated)\n",
    "  - same_answer - bool marked by human stating if the human_answer and rag_answer are the same\n",
    "\n",
    "Your goal is to optimize the **prompt** of the evaluator model in a way that when provided with question, human_answer, and rag_answer returns the same cathegory as human did in `same_answer`.\n",
    "\n",
    "**Let's first get acquainted with the chain for the Evaluator model**. Model expects question, ground_truth and generated_answer as an input into the prompt. The format_instructions variable is handled automatically by langchain. Output of the model is a dictionary with key correct_answer holding bool value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROMPT TEMPLATE FOR THE EVALUATOR\n",
    "# template needs to include {question}, {ground_truth}, {generated_answer}, and {format_instructions} where the context\n",
    "# required by\n",
    "compare_prompt_template = \"\"\"You are given a question, perfect answer and a candidate answer.\n",
    "Your task is to determine if the candidate answer is almost as good as the perfect answer.\n",
    "\n",
    "question:\n",
    "{question}\n",
    "\n",
    "perfect answer (ground truth):\n",
    "{ground_truth}\n",
    "\n",
    "candidate answer:\n",
    "{generated_answer}\n",
    "\n",
    "Format instructions:\n",
    "{format_instructions}\"\"\"\n",
    "\n",
    "\n",
    "def get_compare_chain() -> Runnable:\n",
    "    \"\"\"Langchain Runnable for comparing two answers\n",
    "\n",
    "    Requirements for input dictionary keys:\n",
    "        - question: str - question user asked\n",
    "        - ground_truth: str - Baseline correct answer\n",
    "        - generated_answer: str - Answer which we want to compare to the correct answer\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # define the output parses\n",
    "    class Candidate(BaseModel):\n",
    "        correct_answer: bool = Field(\n",
    "            description=\"is the candidate answer same as perfect answer?\"\n",
    "        )\n",
    "\n",
    "    parser = JsonOutputParser(pydantic_object=Candidate)\n",
    "\n",
    "    # prompt for the comparison model\n",
    "\n",
    "    # define prompt and model\n",
    "    prompt = PromptTemplate(\n",
    "        template=compare_prompt_template,\n",
    "        input_variables=[\"question\", \"ground_truth\", \"generated_answer\"],\n",
    "        partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "    )\n",
    "    model = AzureChatOpenAI(\n",
    "        openai_api_key=api_key,\n",
    "        azure_endpoint=api_endpoint,\n",
    "        azure_deployment=ans_model_type,\n",
    "        openai_api_version=api_version,\n",
    "        temperature=0,\n",
    "        tiktoken_model_name=\"cl100k_base\",\n",
    "    )\n",
    "\n",
    "    # chain\n",
    "    chain = prompt | model | parser\n",
    "    return chain\n",
    "\n",
    "\n",
    "# prepare the chain\n",
    "compare_chain = get_compare_chain()\n",
    "\n",
    "\n",
    "@retry_on_exception\n",
    "def compare_chain_with_retry(input_dict):\n",
    "    return compare_chain.invoke(input_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of compare runtime\n",
    "# compare chain dictionary\n",
    "compare_input_dict = {\n",
    "    \"question\": \"Who is the president of Czech Republic?\",\n",
    "    \"ground_truth\": \"Petr Pavel is prezident of Czech Republic.\",\n",
    "    \"generated_answer\": \"Miloš Zeman is the current president of Czechia.\",\n",
    "}\n",
    "# run the chain\n",
    "compare_answer = compare_chain_with_retry(compare_input_dict)\n",
    "\n",
    "# print answer\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Ground trurh: {compare_input_dict['ground_truth']}\")\n",
    "print(f\"Generated answer: {compare_input_dict['generated_answer']}\")\n",
    "print(\"-------\")\n",
    "print(\n",
    "    f\"Is the generated answer same as ground truth? {compare_answer['correct_answer']}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the evaluator model\n",
    "\n",
    "Training the evaluator model in this case is simply optimizing the prompt in a variable `compare_prompt_template` to achieve ideally same performance as the human evaluator from the provided dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data\n",
    "edf = pd.read_csv(\"./data/validation_evaluator_model.csv\", sep=\",\")\n",
    "edf.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running the LLM evaluator\n",
    "edf[\"ai_same_answer\"] = None\n",
    "for index, row in edf.iterrows():\n",
    "    try:\n",
    "        # get answer\n",
    "        compare_input_dict = {\n",
    "            \"question\": row[\"question\"],\n",
    "            \"ground_truth\": row[\"human_answer\"],\n",
    "            \"generated_answer\": row[\"rag_answer\"],\n",
    "        }\n",
    "        # run the chain\n",
    "        compare_answer = compare_chain_with_retry(compare_input_dict)\n",
    "        # save the results\n",
    "        edf.loc[index, \"ai_same_answer\"] = compare_answer[\"correct_answer\"]\n",
    "    except Exception as err:\n",
    "        print(f\"Row {index}, Exception raised as {err}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing the evaluator results to baseline\n",
    "def rate_evaluator(edf: pd.DataFrame):\n",
    "    \"\"\"Function for showing report about the evaluator\"\"\"\n",
    "    edf[\"ai_same_answer\"] = edf[\"ai_same_answer\"].astype(bool)\n",
    "    used_labels = edf[\"ai_same_answer\"].unique()\n",
    "    cm = confusion_matrix(\n",
    "        edf[\"same_answer\"],\n",
    "        edf[\"ai_same_answer\"],\n",
    "        labels=used_labels,\n",
    "    )\n",
    "    # define figures\n",
    "    fig, ax = plt.subplots(figsize=(9, 4), nrows=1, ncols=2)\n",
    "    fig.suptitle(\"Evaluator score\\n(Comparison evaluator generated labels to human)\")\n",
    "    values = [\n",
    "        len(edf[edf[\"same_answer\"] == edf[\"ai_same_answer\"]]),\n",
    "        len(edf[edf[\"same_answer\"] != edf[\"ai_same_answer\"]]),\n",
    "    ]\n",
    "    # piechart w\n",
    "    ax[0].pie(\n",
    "        values,\n",
    "        labels=[\"Match\", \"Miss\"],\n",
    "        autopct=make_autopct(values),\n",
    "        startangle=45,\n",
    "        colors=[\"forestgreen\", \"tab:red\"],\n",
    "    )\n",
    "    ax[0].axis(\"equal\")\n",
    "\n",
    "    ax[1].imshow(cm, cmap=\"summer\")\n",
    "    ax[1].set_xticks(np.arange(len(used_labels)))\n",
    "    ax[1].set_yticks(np.arange(len(used_labels)))\n",
    "    ax[1].set_xticklabels(used_labels, rotation=90)\n",
    "    ax[1].set_yticklabels(used_labels)\n",
    "    ax[1].set_xlabel(\"Evaluator predicted label\")\n",
    "    ax[1].set_ylabel(\"Human label\")\n",
    "\n",
    "    for i in range(len(used_labels)):\n",
    "        for j in range(len(used_labels)):\n",
    "            ax[1].text(j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"k\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(classification_report(edf[\"same_answer\"], edf[\"ai_same_answer\"]))\n",
    "    pass\n",
    "\n",
    "\n",
    "human_acc = len(edf[edf[\"same_answer\"] == True]) / len(edf)\n",
    "ai_acc = len(edf[edf[\"ai_same_answer\"] == True]) / len(edf)\n",
    "print(f\"Human expert evaluated accuracy as: {human_acc:0,.2f}\")\n",
    "print(f\"Automated evaluator evaluated accuracy as: {ai_acc:0,.2f}\")\n",
    "print(f\"Absolute error of evaluator: {abs(ai_acc-human_acc):0,.2f}\")\n",
    "print(\"\\nMore detailed evaluator accuracy: \")\n",
    "rate_evaluator(edf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Session 4: Optimizing the RAG application\n",
    "\n",
    "\n",
    "In the last session, your goal is to optimize the RAG application using your trained evaluator model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/validation_rag_questions.csv\", sep=\",\")\n",
    "# search types\n",
    "# maximum marginal relevance\n",
    "search_mmr = {\"type\": \"mmr\", \"num_docs\": 1, \"lambda_mult\": 1}\n",
    "# vector similarity\n",
    "search_vec = {\"type\": \"vector\", \"num_docs\": 5, \"similarity_treshold\": 0}\n",
    "\n",
    "df[\"rag_answer\"] = \"empty\"\n",
    "df[\"is_rag_correct\"] = None\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    try:\n",
    "        # get answer\n",
    "        rag_input_dict = {\"question\": row[\"question\"], \"search\": search_vec}\n",
    "        rag_answer = rag_chain_with_retry(temperature=0, input_dict=rag_input_dict)\n",
    "\n",
    "        # compare chain\n",
    "        compare_input_dict = {\n",
    "            \"question\": row[\"question\"],\n",
    "            \"ground_truth\": row[\"correct_answer\"],\n",
    "            \"generated_answer\": rag_answer[\"answer\"],\n",
    "        }\n",
    "        # run the chain\n",
    "        compare_answer = compare_chain_with_retry(compare_input_dict)\n",
    "\n",
    "        # save the results\n",
    "        df.loc[index, \"rag_answer\"] = rag_answer[\"answer\"]\n",
    "        df.loc[index, \"is_rag_correct\"] = compare_answer[\"correct_answer\"]\n",
    "\n",
    "        print(f\"Row {index} compared\")\n",
    "    except Exception as err:\n",
    "        print(\n",
    "            f\"WARNING: Row {index}, Exception raised as {err}, Evaluator answer: {compare_answer}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General RAG performance\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(8, 5))\n",
    "fig.suptitle(\"Automated evaluation\")\n",
    "ax[0].set_title(\"Was RAG correct?\")\n",
    "ax[0].pie(\n",
    "    df[\"is_rag_correct\"].value_counts(),\n",
    "    labels=df[\"is_rag_correct\"].value_counts().index,\n",
    "    autopct=make_autopct(df[\"is_rag_correct\"].value_counts()),\n",
    "    startangle=45,\n",
    "    colors=[\"forestgreen\", \"tab:red\"],\n",
    ")\n",
    "cdf = df.dropna(subset=\"article\")\n",
    "matched = cdf.apply(\n",
    "    lambda row: row[\"rag_answer\"].lower().find(row[\"article\"].lower()) != -1, axis=1\n",
    ").sum()\n",
    "values = [matched, len(cdf) - matched]\n",
    "ax[1].set_title(\"Did RAG cited correct?\")\n",
    "ax[1].pie(\n",
    "    values,\n",
    "    labels=[\"True\", \"False\"],\n",
    "    autopct=make_autopct(values),\n",
    "    startangle=45,\n",
    "    colors=[\"forestgreen\", \"tab:red\"],\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG performance per question type\n",
    "ncols = 3\n",
    "nrows = 2\n",
    "fig, ax = plt.subplots(ncols=ncols, nrows=nrows, figsize=(12, 10))\n",
    "fig.suptitle(\"Performance per question type\")\n",
    "# go through question types\n",
    "for col in range(ncols):\n",
    "    for row in range(nrows):\n",
    "        type = df[\"Type\"].value_counts().index[col * nrows + row]\n",
    "        sdf = df[df[\"Type\"] == type]\n",
    "        ax[row, col].pie(\n",
    "            sdf[\"is_rag_correct\"].value_counts(),\n",
    "            labels=sdf[\"is_rag_correct\"].value_counts().index,\n",
    "            autopct=make_autopct(sdf[\"is_rag_correct\"].value_counts()),\n",
    "            startangle=45,\n",
    "            colors=[\"forestgreen\", \"tab:red\"],\n",
    "        )\n",
    "        ax[row, col].set_title(f\"Questions: {type}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
