{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data preparation for election RAG app**\n",
    "\n",
    "The following notebook was used to create the chroma database used during the workshop. Please note that it wasn't part of the workshop and we added it for people who are interested in creating their own database with different data. Data used for creating this db are scraped wikipedia articles stored in `./data/scrape-january-2024`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "from langchain_community.document_loaders.dataframe import DataFrameLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from loguru import logger\n",
    "from openai import AzureOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_edit_button(input: str) -> str:\n",
    "    return re.sub(r\"\\[edit <[^]]+>\\]\", \"\", input)\n",
    "\n",
    "\n",
    "def remove_cite_nodes(input: str) -> str:\n",
    "    return re.sub(r\"\\^\\[\\d+\\]\", \"\", input)\n",
    "\n",
    "\n",
    "def remove_references(input: str) -> str:\n",
    "    return re.sub(r\"\\s*References\\s*([\\s\\S]*)\", \"\", input)\n",
    "\n",
    "\n",
    "def remove_wiki_links(input: str) -> str:\n",
    "    return re.sub(r\"<[^>]*>\", \"\", input)\n",
    "\n",
    "\n",
    "def slice_data(input: str) -> pd.DataFrame:\n",
    "    \"\"\"Slices the data into a dataframe where rows are chronological and have specified\n",
    "    type\"\"\"\n",
    "    slices = input.split(\"\\n\\n\")\n",
    "\n",
    "    d = []\n",
    "    for i, slice in enumerate(slices):\n",
    "        if i == 0:\n",
    "            d.append({\"type\": \"article_name\", \"text\": slice, \"len\": len(slice)})\n",
    "        elif slice[:7] == \"\\n      \":\n",
    "            d.append({\"type\": \"subchapter\", \"text\": slice, \"len\": len(slice)})\n",
    "        elif slice[:5] == \"\\n    \":\n",
    "            d.append({\"type\": \"chapter\", \"text\": slice, \"len\": len(slice)})\n",
    "        else:\n",
    "            d.append({\"type\": \"text\", \"text\": slice, \"len\": len(slice)})\n",
    "    return pd.DataFrame(d)\n",
    "\n",
    "\n",
    "def prepare_sections(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Takes the dataframe of ordered article and splits it into final sections\"\"\"\n",
    "\n",
    "    article = df[\"text\"][0]\n",
    "    c_chapter = \"\"\n",
    "    c_subchapter = \"\"\n",
    "    c_text = \"\"\n",
    "\n",
    "    sections = []\n",
    "\n",
    "    for row in df[1:].itertuples():\n",
    "        # if row is text type, adds it into section. Finishes section if added text is\n",
    "        # too long\n",
    "        if row.type == \"text\":\n",
    "            c_text += row.text\n",
    "            if len(c_text) > 1500:\n",
    "                sections.append(\n",
    "                    {\n",
    "                        \"article\": article,\n",
    "                        \"chapter\": c_chapter,\n",
    "                        \"subchapter\": c_subchapter,\n",
    "                        \"text\": c_text.lstrip(),\n",
    "                        \"length\": len(c_text.lstrip()),\n",
    "                    }\n",
    "                )\n",
    "                c_text = \"\"\n",
    "\n",
    "        # if type is chapter, finishes previous section and restarts the values\n",
    "        elif row.type == \"chapter\":\n",
    "            sections.append(\n",
    "                {\n",
    "                    \"article\": article,\n",
    "                    \"chapter\": c_chapter,\n",
    "                    \"subchapter\": c_subchapter,\n",
    "                    \"text\": c_text.lstrip(),\n",
    "                    \"length\": len(c_text.lstrip()),\n",
    "                }\n",
    "            )\n",
    "            c_chapter = row.text.lstrip()\n",
    "            c_text = \"\"\n",
    "\n",
    "        # if type is subchapter, finishes previous section and restarts the values\n",
    "        elif row.type == \"subchapter\":\n",
    "            sections.append(\n",
    "                {\n",
    "                    \"article\": article,\n",
    "                    \"chapter\": c_chapter,\n",
    "                    \"subchapter\": c_subchapter,\n",
    "                    \"text\": c_text.lstrip(),\n",
    "                    \"length\": len(c_text.lstrip()),\n",
    "                }\n",
    "            )\n",
    "            c_subchapter = row.text.lstrip()\n",
    "            c_text = \"\"\n",
    "\n",
    "    df = pd.DataFrame(sections)\n",
    "    # throws away all sections which have no text\n",
    "    df = df[df[\"length\"] > 0]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def split_text_at_sentence_end(text, min_length=2500, overlap=200):\n",
    "    \"\"\"Function to split text at the sentence end nearest to the middle,\n",
    "    with at least 200 characters overlap\"\"\"\n",
    "    if len(text) <= min_length:\n",
    "        return [text]\n",
    "\n",
    "    # Find all sentence ends in the text\n",
    "    sentence_ends = [m.start(0) for m in re.finditer(r\"\\.\\s+\", text)]\n",
    "\n",
    "    # Find the best place to split the text\n",
    "    split_point = None\n",
    "    for end in sentence_ends:\n",
    "        if min_length / 2 - overlap <= end <= min_length / 2 + overlap:\n",
    "            split_point = end\n",
    "            break\n",
    "\n",
    "    # If no suitable split point found, force a split at the minimum length with overlap\n",
    "    if split_point is None:\n",
    "        split_point = min_length - overlap\n",
    "\n",
    "    return [text[: split_point + 1].strip(), text[split_point + 1 :].strip()]\n",
    "\n",
    "\n",
    "def split_long_sections(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    new_rows = []  # List to hold the new rows\n",
    "\n",
    "    # Iterate over each row in the DataFrame\n",
    "    for index, row in df.iterrows():\n",
    "        if row[\"length\"] > 2500:\n",
    "            # Split the text into two parts\n",
    "            parts = split_text_at_sentence_end(row[\"text\"])\n",
    "            for part in parts:\n",
    "                # Create a new row with the same data except for the split text\n",
    "                new_row = row.to_dict()\n",
    "                new_row[\"text\"] = part\n",
    "                new_row[\"length\"] = len(part)\n",
    "                new_rows.append(new_row)\n",
    "        else:\n",
    "            # If the length is not greater than 2500, keep the row as is\n",
    "            new_rows.append(row.to_dict())\n",
    "\n",
    "    # Create a new DataFrame from the list of new rows\n",
    "    new_df = pd.DataFrame(new_rows)\n",
    "    return new_df\n",
    "\n",
    "\n",
    "def data_pipeline(file_path: str):\n",
    "    # STEP 1 - load txt file into str\n",
    "    with open(file=file_path, mode=\"r\") as file:\n",
    "        data_str = file.read()\n",
    "\n",
    "    # STEP 2 - cleaning of the data from balast\n",
    "    data_str = remove_edit_button(data_str)\n",
    "    data_str = remove_cite_nodes(data_str)\n",
    "    data_str = remove_references(data_str)\n",
    "    data_str = remove_wiki_links(data_str)\n",
    "\n",
    "    # STEP 3 - Slicing the data into dataframe\n",
    "    df = slice_data(data_str)\n",
    "    sections = prepare_sections(df)\n",
    "\n",
    "    # STEP 4 - Final cleanup and check\n",
    "    sections = split_long_sections(sections)\n",
    "    sections[\"text\"] = (\n",
    "        \"Article: \"\n",
    "        + sections[\"article\"]\n",
    "        + \", Chapter: \"\n",
    "        + sections[\"chapter\"]\n",
    "        + \", Subchapter: \"\n",
    "        + sections[\"subchapter\"]\n",
    "        + \", Text: \"\n",
    "        + sections[\"text\"]\n",
    "    )\n",
    "\n",
    "    logger.debug(f\"Created {len(df)} sections for {os.path.basename(file_path)}\")\n",
    "    return sections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Runtime**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_files_in_folder(folder_path):\n",
    "    file_paths = []\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            file_paths.append(os.path.join(root, file))\n",
    "    return file_paths\n",
    "\n",
    "\n",
    "paths = list_files_in_folder(\"./data/scrape-january-2024\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = []\n",
    "for path in paths:\n",
    "    articles.append(data_pipeline(path))\n",
    "\n",
    "df = pd.concat(articles, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# infromation\n",
    "logger.debug(f\"Longest section have {df['length'].max()} characters\")\n",
    "logger.debug(f\"Final datatset has {len(df)} sections.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embedding(text: str) -> list:\n",
    "    emb_client = AzureOpenAI(\n",
    "        azure_endpoint=os.environ.get(\"EMBEDDING_ENDPOINT\"),\n",
    "        api_key=os.environ.get(\"EMBEDDING_API_KEY\"),\n",
    "        api_version=os.environ.get(\"EMBEDDING_API_VERSION\"),\n",
    "    )\n",
    "    response = emb_client.embeddings.create(\n",
    "        input=text, model=os.environ.get(\"EMBEDDING_NAME\")\n",
    "    )\n",
    "    return response.data[0].embedding\n",
    "\n",
    "\n",
    "df[\"embedding\"] = df[\"text\"].apply(compute_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"./data/processed_wiki.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**VectorStore**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = AzureOpenAIEmbeddings(\n",
    "    api_key=os.environ.get(\"EMBEDDING_API_KEY\"),\n",
    "    azure_endpoint=os.environ.get(\"EMBEDDING_ENDPOINT\"),\n",
    "    api_version=os.environ.get(\"EMBEDDING_API_VERSION\"),\n",
    "    model=\"embeddings-ada\",\n",
    ")\n",
    "\n",
    "loader = DataFrameLoader(data_frame=df, page_content_column=\"text\")\n",
    "documents = loader.load()\n",
    "\n",
    "db = Chroma.from_documents(documents, embedding_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
